{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with customly trained SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.patches import Patch\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.datamodule import SegmentationDataset\n",
    "from src.pl_module import SegformerConfig, SegformerForSemanticSegmentation\n",
    "from utils import LABEL_COLORS, label2rgb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample image\n",
    "img_dir = \"../data/025_08.jpg\"\n",
    "\n",
    "pil_img = Image.open(img_dir)\n",
    "print(pil_img)\n",
    "img_transforms = SegmentationDataset.segformer_img_tfms\n",
    "\n",
    "img_in = img_transforms(pil_img)\n",
    "img_in = img_in.unsqueeze(0)  # Add batch dimension\n",
    "print(img_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint\n",
    "log_dir = Path(\"../logs_pl/mit-b0/version_0\")\n",
    "ckpt_file = \"best_model.safetensors\"\n",
    "\n",
    "config = SegformerConfig.from_json_file(log_dir / \"model_config.json\")\n",
    "model = SegformerForSemanticSegmentation(config)\n",
    "\n",
    "# torch.load has security issues, use safetensors instead\n",
    "# ckpt = torch.load(log_dir / ckpt_fp, weights_only=True)\n",
    "# print([key for key in ckpt[\"state_dict\"].keys() if \"decode_head\" in key])\n",
    "\n",
    "# # Remove \"model.\" prefix from checkpoint keys\n",
    "# state_dict = {key.replace(\"model.\", \"\"): value for key, value in ckpt[\"state_dict\"].items()}\n",
    "\n",
    "state_dict = {}\n",
    "with safe_open(log_dir / ckpt_file, framework=\"pt\", device=\"cpu\") as f:\n",
    "    for key in f.keys():\n",
    "        state_dict[key] = f.get_tensor(key)\n",
    "\n",
    "# print(model.state_dict().keys())\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "model.eval()\n",
    "img_in = img_in.to(device)\n",
    "model.to(device)\n",
    "outs = model.forward(pixel_values=img_in, labels=None)\n",
    "logits = outs.logits  # shape (batch_size, num_labels, ~height/4, ~width/4)\n",
    "\n",
    "print(logits.shape)\n",
    "print(pil_img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust logits\n",
    "# resize output to match input image dimensions\n",
    "upsampled_logits = torch.nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=pil_img.size,\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,  # H x W\n",
    ")\n",
    "\n",
    "# get label masks\n",
    "labels = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "# move to CPU to visualize in matplotlib\n",
    "labels_viz = labels.cpu().numpy()\n",
    "print(labels_viz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a legend\n",
    "legend_patches = [\n",
    "    Patch(color=np.array(rgb), label=label)  # Normalize RGB to [0, 1] for matplotlib\n",
    "    for rgb, label in zip(list(LABEL_COLORS.values()), list(LABEL_COLORS.keys()))\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs: list[plt.Axes]\n",
    "\n",
    "axs[0].imshow(pil_img)\n",
    "axs[0].set_title(\"Image\")\n",
    "axs[1].imshow(label2rgb(labels_viz))\n",
    "axs[1].set_title(\"Pred Mask\")\n",
    "axs[1].legend(\n",
    "    handles=legend_patches, bbox_to_anchor=(1.05, 1.05), loc=\"upper left\", title=\"Classes\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
